# Daily Note: 2022 April 19

start working at 1829 to 2235 (after came back from the lab session of Medical Imaging Analysis).

### Aim

- complete the schedule for the next 8 weeks.
- complete analyzing the statistics of the entire dataset (including `test-challenge`, `test-dev`, `train` and `val`).
  - The percent of coverage of bounding box.
  - prepare essential information for analyzing sparsity. I don't know how to examine this in a mathematical way.

### Work

- write the planning (located in `~/note/daily/planning.md`).
- clean up `.gitignore` and work on `drejo` (I forgot to bring my laptop charger).
- merged my previous notes in `~/readme.md` (basically solution to installation errors). 
- write `~/code/dataset_statistics/bbox_coverage.py` for evaluating bounding box coverage over the entire dataset (not completed). 
  - realized based on the code `image_split.py`.
  - Difficulty in taking union of all bounding boxes in one image. I wonder whether this is a problem, because I think that "the union of all bounding boxes in one single image is 0 because of the usage of NMS". This might be wrong. Now just take the *area sum* of every bounding box in every image instead of using *area union*. This is of course not strictly right. 
  - leave the entry for arguments for further investigation.
- write some notes about the problem statement in `~/note/thesis/1_problem_statement.md` and the whiteboard along with some assumptions I need to verify via experiment. Looking forward to discuss about the assumptions!

### Questions

- I think the training set given is a little bit small, especially on large scale images. The training set contains 419 original images, and after splitting, there are 3838 sub images. 
  - setup. sliding window, default window size 1024, default gap 0.5. Default: just as the DOTA paper. 
  - Splitting details. After splitting, part 4: 2102; part 5: 1432; part 6: 304.
  - Double checked the files both in the Baidu Pan and oneDrive, they are the same, only these three parts (4, 5, 6) are included in the training set. 
  - Additionally, the number of training `labelTxt` they gave is 1830, not equal to 419. But there is a piece of code to do the mapping, so I think it is not a problem.
  - However, the major problem is: The number of **really large images** are not sufficient to train. The number of images whose resolution is larger than `1024*1024` is only 54 (part 4: 18; part 5: 30; part 6: 6). 
- The lack of very large images in the training set is a major problem. The 


### A brief modification of theproblem statement

If I have a prior knowledge of (or dynamically determine) the object size, then I can use this as an input to the network to determine the proper window size. 

- By doing this, I can alleviate the problem of "insufficiency of very large image" in the training set.

## Further work 

- Add object size statistics.
